{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykYHgsNxOAyn","outputId":"271d6437-d6b8-49be-cdca-36f8e6c335db"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/cosmoquester/aibookathon/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer"]},{"cell_type":"markdown","metadata":{"id":"XvfQSgSRsPXs"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YDcVSJIOAyr"},"outputs":[],"source":["# 학습에 사용했던 요약문을 생성할 때와 똑같은 요약 모델과 생성 옵션을 사용하는 것이 좋습니다.\n","summarize_model_name: str = \"digit82/kobart-summarization\"\n","summarize_num_beams: int = 4\n","sum_input_max_length: int = 512\n","sum_max_token_length: int = 128\n","\n","model_name: str = \"khu-bot/polyglot-essayist-with-sum\"\n","tokenizer_name: str = model_name\n","# Private으로 huggingface model hub에 올라가 있는 경우에 필요합니다.\n","auth_token: str = None\n","prompt_max_length: int = 128\n","max_length: int = 512\n","device: str = \"cuda\"\n","\n","# 이 개수만큼의 글자를 생성하면 종료됩니다.\n","num_generate_characters: int = 2000\n","# 추론에 몇 개의 요약문을 사용할지를 명시합니다.\n","use_n_recent_summarizations: int = 10"]},{"cell_type":"markdown","metadata":{"id":"44-5-jJ0sPXu"},"source":["# Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tyW9g5YBOAys"},"outputs":[],"source":["tokenizer_kwargs = {\"padding_side\": \"left\", \"truncation_side\": \"left\"}\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_auth_token=auth_token, **tokenizer_kwargs)\n","model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=auth_token).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNX1SOLQIHDe","outputId":"7ca7abfa-fc90-4cb8-fb37-d2074a25d1c7"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]}],"source":["tokenizer_kwargs = {\"padding_side\": \"left\", \"truncation_side\": \"left\"}\n","summarize_tokenizer = AutoTokenizer.from_pretrained(summarize_model_name, use_auth_token=auth_token, **tokenizer_kwargs)\n","summarize_model = AutoModelForSeq2SeqLM.from_pretrained(summarize_model_name).to(device)"]},{"cell_type":"markdown","metadata":{"id":"DZCPMGV9sPXv"},"source":["# Initial Condition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sm1IcJudsPXv"},"outputs":[],"source":["# 같은 title과 seed를 사용하면 결과를 재현할 수 있습니다. (물론 config도)\n","title: str = \"나의 빛과 어둠\"\n","seed: int = 42\n","backup_indices = [7, 0, 8, 11, 14, 11, 0, 9, 0, 1, 6] # 기존에 title과 seed로 생성한 글의 selected indices를 넣으시면 같은 결과를 낼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"v434WF4osPXw"},"source":["# Write"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOkX_dycsPXw"},"outputs":[],"source":["def datum_to_string(datum, use_n_summary: int = 10):\n","    text = f\"제목: {datum['title']}\\n\"\n","\n","    summarizations = datum.get(\"summarizations\")\n","    if summarizations:\n","        summarizations = summarizations[-use_n_summary:]\n","        summarization = \" \".join(summarizations).replace(\"\\n\", \" \")\n","        text = f\"요약: {summarization}\\n\" + text\n","    return text\n","\n","def segment_by_anychar(text: str, delimiters: str):\n","    \"\"\"delimiters로 주어진 글자 중 어느 하나라도 나온다면 끊어서 segment로 나눠줍니다.\"\"\"\n","    segments = []\n","    last_index = 0\n","    for i, char in enumerate(text):\n","        if char in delimiters:\n","            segments.append(text[last_index: i + 1])\n","            last_index = i + 1\n","    return segments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeBzsj4XOAyt"},"outputs":[],"source":["initial_input_example = {\n","    \"title\": title, \n","    \"summarizations\": [], \n","    \"contents\": []\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHgFEsQEOAyu","outputId":"17e82140-5916-4c84-b306-78c6f95f66eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Indices: [7, 0, 8, 11, 14, 11, 0, 9, 0, 1, 6]\n","\n","[TITLE]\n","나의 빛과 어둠\n","\n","[SUMMARIZATIONS]\n","<1> 나의 삶과 시가 온통 어둡고 칙칙한 색깔인 이유는 당연하며 나의 삶과 시가 온통 어둡고 칙칙한 색깔인 이유는 당연하다.\n","<2> 삶은 언제나 찬란하고 아름다웠으면 좋겠지만 삶을 살고 있는 나의 현실은 그러하지 못하다.\n","<3> 나여서 나여서 나란 녀석은 이 세상 곳곳에 불빛을 발하고 있으 보이 보이 보이 보이 보이지만 나여서 나란 녀석은 이 세상 곳곳에 불빛을 발하고 있으 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이 보이\n","<4> 후회하지 사람은 있어도 후회하지 않는 삶을 사는 사람은 아마 몇 없을 것이다.\n","<5> 누군가의 비교할 수밖에 없는 입장이 되니 가끔은 다른 이의 인생에 질투도 하고, 나만의 빛은 초라하게 느껴진다.\n","<6> 이 어둠은 언제고 다시 찾아 올 준비를 하고 있다는 것을 알기 때문에 더 열심히 빛나려고 노력해야 한다.\n","<7> 언젠가는 내 것인 줄 알았던 빛을 잃게 되고, 빛과 어둠을 모두 가질 순간을 위해 빛과 어둠을 더 열심히 받아들여야 한다.\n","<8> 빛이 없던 자리, 어둠이 가득하던 자리, 그 자리를 빛으로 채운 사람은 이미 어둠 속에서도 빛을 본다.\n","\n","[KEEP CONTENT]\n","나의 빛과 어둠\n","\"내 나이 열여섯 살에 나는 너무 어렸다.\"\n","시인의 말처럼 나야말로 열여섯 살에 시를 쓸 수 있을 거라곤 전혀 생각하지 못하고 살았다. 그러니 나의 삶과 시가 온통 어둡고 칙칙한 색깔인 이유는 당연하다.그 시절은 아직 어두웠고, 나는 여전히 칠흑 같은 어둠 속에서 헤매고 있다. ‘나’를 찾아가는 것이 얼마나 힘든 일인가. 나의 고통 또한 얼마나 고통스럽고 답답했던가. 나는 알았다. 어둠이 계속되는 건, 삶이 계속되는 건 얼마나 외롭고 힘든 일인가. 삶은 언제나 찬란하고 아름다웠으면 좋겠지만 삶을 살고 있는 나의 현실은 그러하지 못하다. 나는 살아야 한다.그럼에도 괜찮다. 내가 나여서 나란 녀석은 이 세상 곳곳에 불빛을 발하고 있으니까.\n","빛나고 아름답기까지 하니까.\n","나는 나여서 나이기에 빛날 수 있는 것이고\n","삶이 있기에 아름다운 것이고\n","빛과 어둠이 있기에 찬란한 것이란 걸 인정하기까지\n","나는 참으로 오래 걸렸다. 아직도 낯설고 서툴다.\n","살면서 자신의 삶을 후회하지 않는 사람은 있어도 후회하지 않는 삶을 사는 사람은 아마 몇 없을 것이다. 물론 모든 후회는 부정적인 영향을 끼친다. 그러나 내게 있었던 후회들은 돌이켜보면, 나의 삶에 대한 책임과 존중의 메시지를 전달해 왔다는 점이다. 모든 후회는 내가 짊어져야 할 인생의 일부분이 분명하다. 그리고 나 스스로에게도 그것을 알려주고 싶었는지도 모르겠다. 나는 스스로를 너무도 모르는 사람이었고, 내 생각을 읽는 사람은 아무도 없었다. 그래서 나는 나 자신의 삶을 더욱 사랑하게 됐다. 그것은 나를 더 성장시킬 수 있도록 만드는 원동력이 되기도 했다.\n","하지만 스스로를 사랑하기 위해 삶을 사랑하지 않을 이유는 없다. 나는 내 영혼이 존재하고 있다고 믿는다. 그리고 이것을 다른 이들에게도 알려주고 싶다. 우리가 겪는 어둠 속에서 밝은 빛을 발견하는 순간, 어둠은 우리를 결코 비켜가지 않겠지만, 나는 여전히 빛을 비추고 있을 거라고. 그렇게 반짝이는 불빛을 따라가면 결국에는 어느 곳이든 목적지에 닿을 수 있을 거라고.나에게는 내 인생보다 오래된 그림자를 붙잡는 버릇, 그것이 가끔씩 고개를 들곤 한다. 그리고 가끔은 다른 이의 인생에 질투도 하고, 나만의 빛은 초라하게 느껴진다. 내 인생을 생각해보는 날이 없는 건 아니지만 누군가와 비교할 수밖에 없는 입장이 되니 가끔 그 삶이 내 삶에 대해 생각하게 만든다. 그럴 때면 나는 내가 그 삶을 살지 않아서 그럴 수도 있지만, 나는 그냥 이 세상에 있는데 내 인생이 그 자리에 와있어서 그럴 수도 있겠다고 생각한다. 내 인생에는 내가 있겠지만 남들보다는 조금 긴 터널 같은 길을 지나고 있다. 어쩌면 내 인생에는 내가 가장 사랑한다고 생각하는 것조차 남의 것일 수도 있겠다. 하지만 또 다른 내 인생에서 보면 하나의 점이지만 내 인생의 나만의 빛을 찾을 수 있지 않을까. 나의 삶에서 빛이 있듯이 남들의 그림자도 있을 테니까. 나의 삶이 타인의 빛과 그림자를 낳을 테니까.\n","나는 누군가의 삶 안에서 반짝 빛을 내는 일을 하고 싶다.그렇다고 이 시기가 마냥 좋지는 않다. 이 어둠은 언제고 다시 찾아 올 준비를 하고 있다는 것을 알기 때문이다. 그래서 더 열심히 빛나려고 노력해야한다. 아무리 힘들고 우울한 시기가 와서 세상이 회색빛으로 물드더라도, 나의 빛이 남아 있을 수 있는 빛을 내야만 한다.\n","나는 이 암흑을 뚫고 빛을 내야한다. 그리고 나의 삶을 아름답게 꾸며내야 한다. 더 밝고, 더 빛나고 싶다. 그 빛이 결국은 이 세상을 보다 더 따뜻하게, 더 많이 살아갈 수 있게 만들어줄 것이라는 믿음이 있기 때문이다.하지만 언젠가는 내 것인 줄 알았던 빛을 잃게 되고, 빛과 어둠을 모두 가질 순간을 위해 빛과 어둠을 더 열심히 받아들여야 한다.빛을 보고, 빛으로 가면 된다. 빛이 없던 자리, 어둠이 가득하던 자리, 그 자리를 빛으로 채운 사람은 이미 어둠 속에서도 빛을 본다. 빛을 보는 사람은 빛이 가득한 자리에 서 있다. 어둠 속에선 어둠을 알겠지만, 빛 속에선 어둠을 알 수 없다. 빛을 보지 않은 자는 어둠 속에서도 어둠을 볼 수 없고, 빛을 본 자는 어둠 속에서도 빛을 볼 수 있기 때문이다. 빛은 어둠의 자리마저도 빛으로 채우고, 그곳에서도 빛을 볼 수 있는 사람이 된다.\n","\n","[# OF CURRENT TEXT: 2077]\n"]}],"source":["from IPython.display import clear_output\n","from copy import deepcopy\n","\n","torch.manual_seed(seed)\n","input_example = deepcopy(initial_input_example)\n","select_indices = []\n","\n","while True:\n","  clear_output()\n","  print(\"Selected Indices:\", select_indices)\n","  print(\"\\n[TITLE]\")\n","  print(input_example[\"title\"])\n","  print(\"\\n[SUMMARIZATIONS]\")\n","  for i, summarization in enumerate(input_example[\"summarizations\"], start=1):\n","    print(f\"<{i}> {summarization}\")\n","  print(\"\\n[KEEP CONTENT]\")\n","  keep_content = \"\".join(input_example[\"contents\"])\n","  print(keep_content)\n","  print(f\"\\n[# OF CURRENT TEXT: {len(keep_content)}]\")\n","\n","  if len(keep_content) >= num_generate_characters:\n","    break\n","\n","  prompt = datum_to_string(input_example)\n","  input_ids = tokenizer(\n","          [prompt],\n","          add_special_tokens=False,\n","          max_length=prompt_max_length,\n","          padding=\"longest\",\n","          truncation=True,\n","          return_tensors=\"pt\",\n","          return_token_type_ids=False,\n","          return_attention_mask=False,\n","  )[\"input_ids\"].to(device)\n","\n","  output = model.generate(input_ids, \n","      max_length, \n","      do_sample=True,\n","      num_return_sequences=1,\n","      pad_token_id=tokenizer.pad_token_id,\n","      use_cache=True,\n","  )\n","  output = output.squeeze(dim=0)[input_ids.size(1):]\n","\n","  text = tokenizer.decode(output, skip_special_tokens=True)\n","  segments = segment_by_anychar(text, \".?!\\n'\\\"”’\")\n","  segments.insert(0, \"\")\n","\n","  print(\"\\n[CUR SENTENCES]\")\n","  for i, segment in enumerate(segments):\n","    print(f\"[{i}] {segment}\")\n","  print(\"[-1] ※ EXIT: 더 이상 문장을 생성하지 않습니다.\")\n","\n","  if backup_indices:\n","    select_index = backup_indices.pop(0)\n","  else:\n","    while True:\n","      try:\n","        select_index = int(input(\"Select last sentence index: \"))\n","        break\n","      except:\n","        print(\"숫자를 입력해주세요. 그만 생성하시려면 -1을 입력해주세요.\")\n","        continue\n","\n","  select_indices.append(select_index)\n","  if select_index < 0:\n","    break\n","\n","  selected_segment = segments[:select_index + 1]\n","  selected_text = \"\".join(selected_segment)\n","\n","  if not selected_text:\n","    continue\n","\n","  sum_input_text = summarize_tokenizer.bos_token + selected_text + summarize_tokenizer.eos_token\n","  input_example[\"contents\"].append(selected_text)\n","  sum_input_ids = summarize_tokenizer(\n","          [selected_text],\n","          add_special_tokens=False,\n","          max_length=sum_input_max_length,\n","          padding=\"longest\",\n","          truncation=True,\n","          return_tensors=\"pt\",\n","          return_token_type_ids=False,\n","          return_attention_mask=False,\n","  )[\"input_ids\"].to(device)\n","\n","  sum_outputs = summarize_model.generate(sum_input_ids, \n","      sum_max_token_length, \n","      num_beams=summarize_num_beams,\n","      num_return_sequences=1,\n","      eos_token_id=summarize_tokenizer.eos_token_id,\n","      pad_token_id=summarize_tokenizer.pad_token_id,\n","      use_cache=True,\n","  )\n","\n","  summarization = summarize_tokenizer.decode(sum_outputs.squeeze(dim=0), skip_special_tokens=True)\n","  input_example[\"summarizations\"].append(summarization)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImtrXouesPXy","outputId":"02904db6-8e62-4275-d57a-2ffe35068d2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Input Example: {'title': '나의 빛과 어둠', 'summarizations': [], 'contents': []}\n","Seed: 42\n","Backup Indices: [7, 0, 8, 11, 14, 11, 0, 9, 0, 1, 6]\n","※ 위 정보를 이용하면 똑같은 결과를 다시 재현할 수 있습니다.\n"]}],"source":["print(\"Initial Input Example:\", initial_input_example)\n","print(\"Seed:\", seed)\n","print(\"Backup Indices:\", select_indices)\n","print(\"※ 위 정보를 이용하면 똑같은 결과를 다시 재현할 수 있습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUV5aBNWsPXz","outputId":"ba937842-8505-42d8-8b27-51a49a9dae43"},"outputs":[{"name":"stdout","output_type":"stream","text":["※ 혹시나 중간과정을 스킵하고 이어서 추론을 원한다면 현재 `input_example`을 복원하고 seed를 아래의 값으로 설정하세요.\n","Current Seed: 13235115671464550030\n"]}],"source":["print(\"※ 혹시나 중간과정을 스킵하고 이어서 추론을 원한다면 현재 `input_example`을 복원하고 seed를 아래의 값으로 설정하세요.\")\n","print(\"Current Seed:\", torch.seed())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWJYyVznsPXz"},"outputs":[],"source":["import json\n","import os\n","\n","output_path = title + \".json\"\n","\n","if os.path.exists(output_path):\n","    raise ValueError(\"Already existing output path!\")\n","\n","with open(output_path, \"w\") as f:\n","    json.dump({\n","        \"config\": {\n","            \"summarize_model_name\": summarize_model_name,\n","            \"summarize_num_beams\": summarize_num_beams,\n","            \"sum_input_max_length\": sum_input_max_length,\n","            \"sum_max_token_length\": sum_max_token_length,\n","            \"model_name\": model_name,\n","            \"tokenizer_name\": tokenizer_name,\n","            \"prompt_max_length\": prompt_max_length,\n","            \"max_length\": max_length,\n","            \"device\": device,\n","            \"auth_token\": auth_token,\n","            \"num_generate_characters\": num_generate_characters,\n","            \"use_n_recent_summarizations\": use_n_recent_summarizations,\n","        },\n","        \"initial\": {\n","            \"example\": initial_input_example,\n","            \"seed\": seed,\n","        },\n","        \"output\": {\n","            \"selected_indices\": select_indices,\n","            \"example\": input_example,\n","        }\n","    }, f, ensure_ascii=False, indent=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuxuAIZssPXz"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"9672730e64e8a60be15af299c424939eda74a783c931ea854ba4a2827e6ef14d"}}},"nbformat":4,"nbformat_minor":0}
