{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"ykYHgsNxOAyn","outputId":"271d6437-d6b8-49be-cdca-36f8e6c335db"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/cosmoquester/aibookathon/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer"]},{"cell_type":"markdown","metadata":{"id":"XvfQSgSRsPXs"},"source":["# Config"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"0YDcVSJIOAyr"},"outputs":[],"source":["# 학습에 사용했던 요약문을 생성할 때와 똑같은 요약 모델과 생성 옵션을 사용하는 것이 좋습니다.\n","summarize_model_name: str = \"digit82/kobart-summarization\"\n","summarize_num_beams: int = 4\n","sum_input_max_length: int = 512\n","sum_max_token_length: int = 128\n","\n","model_name: str = \"khu-bot/polyglot-essayist-with-sum\"\n","tokenizer_name: str = model_name\n","# Private으로 huggingface model hub에 올라가 있는 경우에 필요합니다.\n","auth_token: str = None\n","prompt_max_length: int = 128\n","max_length: int = 512\n","device: str = \"cuda\"\n","\n","# 이 개수만큼의 글자를 생성하면 종료됩니다.\n","num_generate_characters: int = 2000\n","# 추론에 몇 개의 요약문을 사용할지를 명시합니다.\n","use_n_recent_summarizations: int = 10"]},{"cell_type":"markdown","metadata":{"id":"44-5-jJ0sPXu"},"source":["# Load Model"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tyW9g5YBOAys"},"outputs":[],"source":["tokenizer_kwargs = {\"padding_side\": \"left\", \"truncation_side\": \"left\"}\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_auth_token=auth_token, **tokenizer_kwargs)\n","model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=auth_token).to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNX1SOLQIHDe","outputId":"7ca7abfa-fc90-4cb8-fb37-d2074a25d1c7"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"]}],"source":["tokenizer_kwargs = {\"padding_side\": \"left\", \"truncation_side\": \"left\"}\n","summarize_tokenizer = AutoTokenizer.from_pretrained(summarize_model_name, use_auth_token=auth_token, **tokenizer_kwargs)\n","summarize_model = AutoModelForSeq2SeqLM.from_pretrained(summarize_model_name).to(device)"]},{"cell_type":"markdown","metadata":{"id":"DZCPMGV9sPXv"},"source":["# Initial Condition"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Sm1IcJudsPXv"},"outputs":[],"source":["# 같은 title과 seed를 사용하면 결과를 재현할 수 있습니다. (물론 config도)\n","title: str = \"나의 빛과 어둠\"\n","seed: int = 42\n","backup_indices = [9, 5, 8, 0, 19, 0, 7, 14] # 기존에 title과 seed로 생성한 글의 selected indices를 넣으시면 같은 결과를 낼 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"v434WF4osPXw"},"source":["# Write"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"TOkX_dycsPXw"},"outputs":[],"source":["import re\n","\n","def datum_to_string(datum, use_n_summary: int = 10):\n","    text = f\"제목: {datum['title']}\\n\"\n","\n","    summarizations = datum.get(\"summarizations\")\n","    if summarizations:\n","        summarizations = summarizations[-use_n_summary:]\n","        summarization = \" \".join(summarizations).replace(\"\\n\", \" \")\n","        text = f\"요약: {summarization}\\n\" + text\n","    return text\n","\n","def segment_text(text: str):\n","    \"\"\"텍스트를 여러 개의 segment로 분리합니다.\"\"\"\n","    seperate_indices = [m.end() for m in re.finditer(r\"([.?!'\\\"”’]+(?!\\w)\\n*|\\n+)\\s*\", text)]\n","    if len(text) not in seperate_indices:\n","        seperate_indices = seperate_indices + [len(text)]\n","\n","    start_index = 0\n","    segments = []\n","    for end_index in seperate_indices:\n","        segments.append(text[start_index: end_index])\n","        start_index = end_index\n","    return segments"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"EeBzsj4XOAyt"},"outputs":[],"source":["initial_input_example = {\n","    \"title\": title, \n","    \"summarizations\": [], \n","    \"contents\": []\n","}"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"bHgFEsQEOAyu","outputId":"17e82140-5916-4c84-b306-78c6f95f66eb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Selected Indices: [9, 5, 8, 0, 19, 0, 7, 14]\n","\n","[TITLE]\n","나의 빛과 어둠\n","\n","[SUMMARIZATIONS]\n","<1> 대학 실기 시험, 대학 입학시험, 대학 수업 그리고 고3 수련회뿐인 나의 삶과 시가 온통 어둡고 칙칙한 색깔인 이유는 당연하다.\n","<2> 순수했고 때 묻지 않은 순수하고 때 묻지 않은 아이가 참 마음에 들었다.\n","<3> 밝고 따뜻한 사람이 되는 것을, 타인의 삶에도 마음을 내어주는 사람이 되는 것을 참 좋은 것이라고 생각하기 시작한 지 이미 오래고, 여전히 그런 것을 꿈꾸고, 여전히 그런 이들을 부러워하고, 그렇게 되지 못할까 봐 슬픈 것입니다.\n","<4> 내가 가진 빛과 어둠을 인정하는 게 쉬운 일은 아니지만, 인정할 것을 인정하자 그 다음부터가 쉬워진다.\n","<5> 내가 갖고 있는 좋은 것의 목록을 헤아려보려면, 나는 그 목록의 마지막을, 그 영역 밖의 무엇이 마지막 칸을 채울 수 있을 것 같다고 생각했던 것에 대해 말한다면, 나는 그 목록의 마지막을, 그 영역 밖의 무엇이 마지막 칸을 채울 수 있을 것 같다.\n","<6> 너무나 사소하고 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들을, 너무나 당연한 모든 것들은, 너무나 쉽게 잊거나 망각하는것들은, 쉽게 만들고, 쉬운 것으로부터 더 쉽게 벗어나고 더 쉽게 벗어나고 쉽게 잊고 잊고 잊고\n","\n","[KEEP CONTENT]\n","나의 빛과 어둠\n","\"내 나이 열여섯 살에 나는 너무 어렸다.\"\n","시인의 말처럼 나야말로 열여섯 살에 시를 쓸 수 있을 거라곤 전혀 생각하지 못하고 살았다. 그러니 나의 삶과 시가 온통 어둡고 칙칙한 색깔인 이유는 당연하다.\n","대학 3학년이 되는 나는 나에게 '고3'이라는 이름표를 붙였다. 학교 선배가 알려준 대로 학교 건물 맨 왼쪽에 있던 강당을 찾아가 나의 고3 출석부를 받았다.\n","이제부터 남은 건 대학 실기 시험, 대학 입학시험, 대학 수업 그리고 고3 수련회뿐이었다. 지금까지와는 뭔가 다르게 내 인생을 보내야 할 것만 같았다.\n","대학을 간다고 생각하니 자꾸 눈물이 났다.\n","처음에는 이 아이가 참 마음에 들었다. 순수했고 때 묻지 않았다. 나는 그 아이에게 내 꿈을 이야기하는 걸 좋아하는데, 그럴 때마다 그 아이는 늘 그렇듯이 늘 자신은 그저 그런 꿈을 꾸는 평범한 아이라고 말했다. 그래서 내 마음은 그냥 그럴 때 있게 사는 것도 괜찮은 거 아니냐며, 너도 꿈이 있고 나도 꿈이 있는 사람이니까 굳이 비교하지 말라고 얘기했다. 그때는 그 아이가 조금은 바보같이 느껴졌는데, 지금 생각해 보면 그냥 내 속마음을 이야기할 수 있는 아이가 필요했던 것 같다.\n","그 시절은 아직 어두웠고, 나는 여전히 마음이 아팠지만, 내가 어떻게 여기고, 내가 무엇에 기뻐하고 슬퍼할 수 있는지도 알았다. 어쩌면 가장 아름다운 마음은, 가장 행복한 상태라는 것은 이미 알고 있었는데. 나도 모르게 나의 마음에 너무 익숙해버리다 보니, 그냥 내가 그런 줄 알았다.\n","지금을 살고 있는 나의 마음도, 그때와 다르지 않다. 밝고 따뜻한 사람이 되는 것을, 타인의 삶에도 마음을 내어주는 사람이 되는 것을 참 좋은 것이라고 생각하기 시작한 지 이미 오래고, 여전히 그런 것을 꿈꾸고, 여전히 그런 이들을 부러워하고, 그렇게 되지 못할까 봐 슬픈 것이다.\n","여전히 그렇다. 나는 여전히 나로 살고 있다. 그렇게 나 자체로 살아가고 싶다.\n","그러니, 내가 내 빛과 어둠을 인정하는 게 그리 어려운 일이 아니다. 그리고 그런 것을 인정하는 게 더 쉽다. 인정할 것을 인정하자 그 다음부터가 쉬워진다. 내가 가진 것들로 내가 살자. 내가 가고 싶은 곳으로 내가 하고 싶은 일을 하자. 내가 그렇게 하고 싶어서 선택한 나의 삶을 살자. 그러다 보면 점점 마음의 크기도 줄어들고, 마음의 무게도 가벼워질 수 있다. 마음에 담지 않고 놓아버릴 수 있다. 나에게 주어진 일을 해내고, 주어진 일을 완수하다 보면, 어느새 나의 삶은 조금씩 빛과 그림자가 희석하다 없어지는 하얀 여백만 덩그러니 남게 될 터다.\n","그렇게 빛을 가득 머금고 빛을 향해 사는 나를 상상한다. 아직도 아주 맑은, 내 눈 앞의 해와 같이 밝은 빛에게. 빛은 언제나 그 자리에 있고 그 빛의 길은 나를 향해 있으니. 언제나 그 자리에서 나를 비춰줄 테니. 언젠가는 빛과 길과 나. 이 모든 게 한 덩이로 합쳐져 눈부시도록 찬란한 빛을 만들지 않을까? 하는 상상이다. 내가 상상하고 그려내는 세상은 언제나 찬란하고 찬란해서, 나에게 남은 빛은 그 어떤 것이라도 품을 수 있을 것 같다. 그러니 나는 아직은 내 인생의 남은 빛을 다 쓸 것이 아니라, 조금씩, 많이 쓸 것이다. 그렇게 쓸 빛이 많았으면 좋겠다.\n","내가 갖고 있는 좋은 것의 목록을 헤아려보았습니다.\n","좋은 것은 마음에 들어, 참 좋습니다. 더 이상 어떤 것을 좋아하고, 말하지 않겠다는 것은 아직은 아닙니다. 아직은, 그런 식으로 내 삶에 어떤 것은 좋은 것이라고 말할 수 있는 가능성은 없을 것 같습니다. 좋다고 말할 수 있는 것이 있을 뿐입니다.\n","다만, 내가 가지고 있는, 내가 좋아하는 빛과 어둠의 범위를 헤아려볼 수 있을 것 같습니다. 이제부터 좋은 것이라고 생각했던 것에 대해 말한다면, 나는 그 목록의 마지막을, 그 영역 밖의 무엇이 마지막 칸을 채울 수 있을 것 같습니다.그것은 마치 그 빛 때문에, 그 어둠 때문에, 그 경계 때문에, 그 한계 때문에, 그 한계 안에서\n","너무나 많은 것이 너무나 사소하고 너무나 당연한 모든 것들을, 너무나 당연한 것들을, 너무나 당연한 모든 것들은, 너무나 쉽게 잊거나 망각하는 것이었구나.\n","너무 쉽게 웃거나 눈물을 흘리기도 하면서\n","나를 쉽게 만들고, 쉬운 것으로부터 더 쉽게 벗어나며\n","쉬운 것들도 쉬이 잊고 잊는데\n","쉬운 것들을 쉽게 기억하기도 하면서\n","그것이 나에게 얼마나 중요한 위치를 차지하는지, 얼마나 큰 의미를 지니는지\n","그것이 나에게 얼마나 큰 절망을 가져다줄 수 있는지\n","나는 왜 그렇게 당연한 것들로부터 얼마나 멀어졌는지,\n","얼마나 자주 잊었는지, 더 자주 잊힐 수밖에 없었는지,\n","그토록 중요한 그 위치는 결국 잊히고 말았던지.\n","너도 기억해줘.\n","나는 그렇게 그 자리에서 많은 날을 보냈다는 것을.\n","너도 잊지 않았으면 좋겠어.\n","\n","\n","[# OF CURRENT TEXT: 2329]\n"]}],"source":["from IPython.display import clear_output\n","from copy import deepcopy\n","\n","torch.manual_seed(seed)\n","input_example = deepcopy(initial_input_example)\n","select_indices = []\n","\n","while True:\n","  clear_output()\n","  print(\"Selected Indices:\", select_indices)\n","  print(\"\\n[TITLE]\")\n","  print(input_example[\"title\"])\n","  print(\"\\n[SUMMARIZATIONS]\")\n","  for i, summarization in enumerate(input_example[\"summarizations\"], start=1):\n","    print(f\"<{i}> {summarization}\")\n","  print(\"\\n[KEEP CONTENT]\")\n","  keep_content = \"\".join(input_example[\"contents\"])\n","  print(keep_content)\n","  print(f\"\\n[# OF CURRENT TEXT: {len(keep_content)}]\")\n","\n","  if len(keep_content) >= num_generate_characters:\n","    break\n","\n","  prompt = datum_to_string(input_example)\n","  input_ids = tokenizer(\n","          [prompt],\n","          add_special_tokens=False,\n","          max_length=prompt_max_length,\n","          padding=\"longest\",\n","          truncation=True,\n","          return_tensors=\"pt\",\n","          return_token_type_ids=False,\n","          return_attention_mask=False,\n","  )[\"input_ids\"].to(device)\n","\n","  output = model.generate(input_ids, \n","      max_length, \n","      do_sample=True,\n","      num_return_sequences=1,\n","      pad_token_id=tokenizer.pad_token_id,\n","      use_cache=True,\n","  )\n","  output = output.squeeze(dim=0)[input_ids.size(1):]\n","\n","  text = tokenizer.decode(output, skip_special_tokens=True)\n","  segments = segment_text(text)\n","  segments.insert(0, \"\")\n","\n","  print(\"\\n[CUR SENTENCES]\")\n","  for i, segment in enumerate(segments):\n","    print(f\"[{i}] {segment}\")\n","  print(\"[-1] ※ EXIT: 더 이상 문장을 생성하지 않습니다.\")\n","\n","  if backup_indices:\n","    select_index = backup_indices.pop(0)\n","  else:\n","    while True:\n","      try:\n","        select_index = int(input(\"Select last sentence index: \"))\n","        break\n","      except:\n","        print(\"숫자를 입력해주세요. 그만 생성하시려면 -1을 입력해주세요.\")\n","        continue\n","\n","  select_indices.append(select_index)\n","  if select_index < 0:\n","    break\n","\n","  selected_segment = segments[:select_index + 1]\n","  selected_text = \"\".join(selected_segment)\n","\n","  if not selected_text:\n","    continue\n","\n","  sum_input_text = summarize_tokenizer.bos_token + selected_text + summarize_tokenizer.eos_token\n","  input_example[\"contents\"].append(selected_text)\n","  sum_input_ids = summarize_tokenizer(\n","          [selected_text],\n","          add_special_tokens=False,\n","          max_length=sum_input_max_length,\n","          padding=\"longest\",\n","          truncation=True,\n","          return_tensors=\"pt\",\n","          return_token_type_ids=False,\n","          return_attention_mask=False,\n","  )[\"input_ids\"].to(device)\n","\n","  sum_outputs = summarize_model.generate(sum_input_ids, \n","      sum_max_token_length, \n","      num_beams=summarize_num_beams,\n","      num_return_sequences=1,\n","      eos_token_id=summarize_tokenizer.eos_token_id,\n","      pad_token_id=summarize_tokenizer.pad_token_id,\n","      use_cache=True,\n","  )\n","\n","  summarization = summarize_tokenizer.decode(sum_outputs.squeeze(dim=0), skip_special_tokens=True)\n","  input_example[\"summarizations\"].append(summarization)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"ImtrXouesPXy","outputId":"02904db6-8e62-4275-d57a-2ffe35068d2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Input Example: {'title': '나의 빛과 어둠', 'summarizations': [], 'contents': []}\n","Seed: 42\n","Backup Indices: [9, 5, 8, 0, 19, 0, 7, 14]\n","※ 위 정보를 이용하면 똑같은 결과를 다시 재현할 수 있습니다.\n"]}],"source":["print(\"Initial Input Example:\", initial_input_example)\n","print(\"Seed:\", seed)\n","print(\"Backup Indices:\", select_indices)\n","print(\"※ 위 정보를 이용하면 똑같은 결과를 다시 재현할 수 있습니다.\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"bUV5aBNWsPXz","outputId":"ba937842-8505-42d8-8b27-51a49a9dae43"},"outputs":[{"name":"stdout","output_type":"stream","text":["※ 혹시나 중간과정을 스킵하고 이어서 추론을 원한다면 현재 `input_example`을 복원하고 seed를 아래의 값으로 설정하세요.\n","Current Seed: 11828976477152541601\n"]}],"source":["print(\"※ 혹시나 중간과정을 스킵하고 이어서 추론을 원한다면 현재 `input_example`을 복원하고 seed를 아래의 값으로 설정하세요.\")\n","print(\"Current Seed:\", torch.seed())"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"MWJYyVznsPXz"},"outputs":[],"source":["import json\n","import os\n","\n","output_path = title + \".json\"\n","\n","if os.path.exists(output_path):\n","    raise ValueError(\"Already existing output path!\")\n","\n","with open(output_path, \"w\") as f:\n","    json.dump({\n","        \"config\": {\n","            \"summarize_model_name\": summarize_model_name,\n","            \"summarize_num_beams\": summarize_num_beams,\n","            \"sum_input_max_length\": sum_input_max_length,\n","            \"sum_max_token_length\": sum_max_token_length,\n","            \"model_name\": model_name,\n","            \"tokenizer_name\": tokenizer_name,\n","            \"prompt_max_length\": prompt_max_length,\n","            \"max_length\": max_length,\n","            \"device\": device,\n","            \"auth_token\": auth_token,\n","            \"num_generate_characters\": num_generate_characters,\n","            \"use_n_recent_summarizations\": use_n_recent_summarizations,\n","        },\n","        \"initial\": {\n","            \"example\": initial_input_example,\n","            \"seed\": seed,\n","        },\n","        \"output\": {\n","            \"selected_indices\": select_indices,\n","            \"example\": input_example,\n","        }\n","    }, f, ensure_ascii=False, indent=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuxuAIZssPXz"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"9672730e64e8a60be15af299c424939eda74a783c931ea854ba4a2827e6ef14d"}}},"nbformat":4,"nbformat_minor":0}
